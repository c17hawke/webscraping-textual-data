{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Author: Sunny Bhaveen Chandra\n",
    "# Contact: sunny.c17hawke@gmail.com\n",
    "# dated: March, 04, 2020\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to gather data\n",
    "data = {\"Product\": list(), \n",
    "      \"Name\": list(), \n",
    "      \"Rating\": list(), \n",
    "      \"CommentHead\": list(), \n",
    "      \"Comment\": list()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_HTML(base_URL=None, search_string=None):\n",
    "\t'''\n",
    "\treturn main html page based on search string\n",
    "\t'''\n",
    "\t# construct the search url with base URL and search string\n",
    "\tsearch_url = f\"{base_URL}/search?q={search_string}\"\n",
    "\t# usung urllib read the page\n",
    "\twith urllib.request.urlopen(search_url) as url:\n",
    "\t    page = url.read()\n",
    "\t# return the html page after parsing with bs4\n",
    "\treturn soup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_name_links(flipkart_base=None, bigBoxes=None):\n",
    "\t'''\n",
    "\treturns list of (product name, product link)\n",
    "\t'''\n",
    "\t# temporary list to return the results\n",
    "\ttemp = []\n",
    "\t# iterate over list of bigBoxes\n",
    "\tfor box in bigBoxes:\n",
    "\t    try:\n",
    "\t    \t# if prod name and list present then append them in temp\n",
    "\t        temp.append((box.div.div.div.a.img['alt'],\n",
    "\t        \tflipkart_base + box.div.div.div.a[\"href\"]))\n",
    "\t    except:\n",
    "\t        pass\n",
    "\t    \n",
    "\treturn temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_HTML(productLink=None):\n",
    "\t'''\n",
    "\treturns each product HTML page after parsing it with soup\n",
    "\t'''\n",
    "\tprod_page = requests.get(productLink)\n",
    "\treturn soup(prod_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(commentbox=None, prodName=None):\n",
    "\t'''\n",
    "\tthis will append data gathered from comment box into data dictionary\n",
    "\t'''\n",
    "\t# append product name\n",
    "\tdata[\"Product\"].append(prodName)\n",
    "\ttry:\n",
    "\t\t# append Name of customer if exists else append default\n",
    "\t    data[\"Name\"].append(commentbox.div.div.\\\n",
    "\t          find_all('p', {'class': '_3LYOAd _3sxSiS'})[0].text)\n",
    "\texcept:\n",
    "\t    data[\"Name\"].append('No Name')\n",
    "\n",
    "\ttry:\n",
    "\t\t# append Rating by customer if exists else append default\n",
    "\t    data[\"Rating\"].append(commentbox.div.div.div.div.text)\n",
    "\texcept:\n",
    "\t    data[\"Rating\"].append('No Rating')\n",
    "\n",
    "\ttry:\n",
    "\t\t# append Heading of comment by customer if exists else append default\n",
    "\t    data[\"CommentHead\"].append(commentbox.div.div.div.p.text)\n",
    "\texcept:\n",
    "\t    data[\"CommentHead\"].append('No Comment Heading')\n",
    "\n",
    "\ttry:\n",
    "\t\t# append comments of customer if exists else append default\n",
    "\t    comtag = commentbox.div.div.find_all('div', {'class': ''})\n",
    "\t    data[\"Comment\"].append(comtag[0].div.text)\n",
    "\texcept:\n",
    "\t    data[\"Comment\"].append('No Customer Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_dataframe(data, fileName=None):\n",
    "\t'''\n",
    "\tit saves the dictionary dataframe as csv by given filename inside\n",
    "\tthe results folder\n",
    "\t'''\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tprint(f\"shape of df: {df.shape}\")\n",
    "\t# create a results folder if not exists\n",
    "\tpath_to_store = 'results'\n",
    "\tos.makedirs(path_to_store, exist_ok=True)\n",
    "\t# save the CSV file to results folder\n",
    "\tdf.to_csv(f\"{path_to_store}/{fileName}.csv\", index=None)\n",
    "\tprint(\"File saved successfully!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing...\n",
      "enter a brandname or a product name: msi\n",
      "shape of df: (70, 5)\n",
      "error detected: name 'os' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\t# get base URL and a search string to query the website\n",
    "\tbase_URL = 'https://www.flipkart.com' # 'https://www.' + input(\"enter base URL: \")\n",
    "\n",
    "    # enter a product name eg \"xiaomi\"\n",
    "\tsearch_string = input(\"enter a brandname or a product name: \")\n",
    "\n",
    "    # fill the spaces between search strings with +\n",
    "\tsearch_string = \"+\".join(search_string.split())\n",
    "\tprint('processing...')\n",
    "\n",
    "    # start counter to count time in seconds\n",
    "\tstart = time.perf_counter()\n",
    "\n",
    "\t# store main HTML page for given search query\n",
    "\tflipkart_HTML = get_main_HTML(base_URL, search_string)\n",
    "\n",
    "\t# store all the boxes containing products\n",
    "\tbigBoxes = flipkart_HTML.find_all(\"div\", {\"class\":\"bhgxx2 col-12-12\"})\n",
    "\n",
    "\t# store extracted product name links\n",
    "\tproduct_name_Links = get_product_name_links(base_URL, bigBoxes)\n",
    "\n",
    "\t# iterate over product name and links list\n",
    "\tfor prodName, productLink in product_name_Links:\n",
    "\t\t# iterate over product HTML\n",
    "\t    for prod_HTML in get_prod_HTML(productLink):\n",
    "\t        try:\n",
    "\t        \t# extract comment boxes from product HTML\n",
    "\t            comment_boxes = prod_HTML.find_all('div', {'class': '_3nrCtb'})\n",
    "\t            # iterate over comment boxes to extract required data\n",
    "\t            for commentbox in comment_boxes:\n",
    "\t            \t# prpare final data\n",
    "\t                get_final_data(commentbox, prodName)\n",
    "\t                \n",
    "\t        except:\n",
    "\t            pass\n",
    "\n",
    "\t# save the data as gathered in dataframe\n",
    "\tsave_as_dataframe(data, search_string)\n",
    "\n",
    "\t# finish time counter and calclulate time taked to complet ethis programe\n",
    "\tfinish = time.perf_counter()\n",
    "\tprint(f\"program finished with and timelapsed: {finish - start} second(s)\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttry:\n",
    "\t\tmain()\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"error detected: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
