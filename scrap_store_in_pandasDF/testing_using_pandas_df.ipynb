{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Author: Sunny Bhaveen Chandra\n",
    "# Contact: sunny.c17hawke@gmail.com\n",
    "# dated: March, 04, 2020\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to gather data\n",
    "data = {\"Product\": list(), \n",
    "      \"Name\": list(), \n",
    "      \"Rating\": list(), \n",
    "      \"CommentHead\": list(), \n",
    "      \"Comment\": list()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_HTML(base_URL=None, search_string=None):\n",
    "    '''\n",
    "    return main html page based on search string\n",
    "    '''\n",
    "    # construct the search url with base URL and search string\n",
    "    search_url = f\"{base_URL}/search?q={search_string}\"\n",
    "    # usung urllib read the page\n",
    "    with urllib.request.urlopen(search_url) as url:\n",
    "        page = url.read()\n",
    "    # return the html page after parsing with bs4\n",
    "    return soup(page, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_name_links(flipkart_base=None, bigBoxes=None):\n",
    "    '''\n",
    "    returns list of (product name, product link)\n",
    "    '''\n",
    "    # temporary list to return the results\n",
    "    temp = []\n",
    "    # iterate over list of bigBoxes\n",
    "    for box in bigBoxes:\n",
    "        try:\n",
    "            # if prod name and list present then append them in temp\n",
    "            temp.append((box.div.div.div.a.img['alt'],\n",
    "                flipkart_base + box.div.div.div.a[\"href\"]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_HTML(productLink=None):\n",
    "    '''\n",
    "    returns each product HTML page after parsing it with soup\n",
    "    '''\n",
    "    prod_page = requests.get(productLink)\n",
    "    return soup(prod_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(commentbox=None, prodName=None):\n",
    "    '''\n",
    "    this will append data gathered from comment box into data dictionary\n",
    "    '''\n",
    "    # append product name\n",
    "    data[\"Product\"].append(prodName)\n",
    "    try:\n",
    "        # append Name of customer if exists else append default\n",
    "        data[\"Name\"].append(commentbox.div.div.\\\n",
    "              find_all('p', {'class': '_3LYOAd _3sxSiS'})[0].text)\n",
    "    except:\n",
    "        data[\"Name\"].append('No Name')\n",
    "\n",
    "    try:\n",
    "        # append Rating by customer if exists else append default\n",
    "        data[\"Rating\"].append(commentbox.div.div.div.div.text)\n",
    "    except:\n",
    "        data[\"Rating\"].append('No Rating')\n",
    "\n",
    "    try:\n",
    "        # append Heading of comment by customer if exists else append default\n",
    "        data[\"CommentHead\"].append(commentbox.div.div.div.p.text)\n",
    "    except:\n",
    "        data[\"CommentHead\"].append('No Comment Heading')\n",
    "\n",
    "    try:\n",
    "        # append comments of customer if exists else append default\n",
    "        comtag = commentbox.div.div.find_all('div', {'class': ''})\n",
    "        data[\"Comment\"].append(comtag[0].div.text)\n",
    "    except:\n",
    "        data[\"Comment\"].append('No Customer Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_dataframe(data, fileName=None):\n",
    "    '''\n",
    "    it saves the dictionary dataframe as csv by given filename inside\n",
    "    the results folder\n",
    "    '''\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"shape of df: {df.shape}\")\n",
    "    # create a results folder if not exists\n",
    "    path_to_store = 'results'\n",
    "    os.makedirs(path_to_store, exist_ok=True)\n",
    "    # save the CSV file to results folder\n",
    "    df.to_csv(f\"{path_to_store}/{fileName}.csv\", index=None)\n",
    "    print(\"File saved successfully!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a brandname or a product name: redmi\n",
      "processing...\n",
      "shape of df: (93, 5)\n",
      "File saved successfully!!\n",
      "program finished with and timelapsed: 7.5536151160013105 second(s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # get base URL and a search string to query the website\n",
    "    base_URL = 'https://www.flipkart.com' # 'https://www.' + input(\"enter base URL: \")\n",
    "\n",
    "    # enter a product name eg \"xiaomi\"\n",
    "    search_string = input(\"enter a brandname or a product name: \")\n",
    "\n",
    "    # fill the spaces between search strings with +\n",
    "    search_string = \"+\".join(search_string.split())\n",
    "    print('processing...')\n",
    "\n",
    "    # start counter to count time in seconds\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # store main HTML page for given search query\n",
    "    flipkart_HTML = get_main_HTML(base_URL, search_string)\n",
    "\n",
    "    # store all the boxes containing products\n",
    "    bigBoxes = flipkart_HTML.find_all(\"div\", {\"class\":\"bhgxx2 col-12-12\"})\n",
    "\n",
    "    # store extracted product name links\n",
    "    product_name_Links = get_product_name_links(base_URL, bigBoxes)\n",
    "\n",
    "    # iterate over product name and links list\n",
    "    for prodName, productLink in product_name_Links:\n",
    "        # iterate over product HTML\n",
    "        for prod_HTML in get_prod_HTML(productLink):\n",
    "            try:\n",
    "                # extract comment boxes from product HTML\n",
    "                comment_boxes = prod_HTML.find_all('div', {'class': '_3nrCtb'})\n",
    "                # iterate over comment boxes to extract required data\n",
    "                for commentbox in comment_boxes:\n",
    "                    # prpare final data\n",
    "                    get_final_data(commentbox, prodName)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # save the data as gathered in dataframe\n",
    "    save_as_dataframe(data, search_string)\n",
    "\n",
    "    # finish time counter and calclulate time taked to complet ethis programe\n",
    "    finish = time.perf_counter()\n",
    "    print(f\"program finished with and timelapsed: {finish - start} second(s)\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"error detected: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
